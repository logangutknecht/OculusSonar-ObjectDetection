# Oculus Sonar Object Detection System

A comprehensive Python-based system for processing, filtering, and detecting objects in Oculus sonar data. This system provides real-time visualization, multiple detection algorithms including **state-of-the-art machine learning**, and advanced filtering capabilities for underwater sonar imagery.

> **ðŸš€ New: Machine Learning Detection** - Train custom YOLOv8 models on your sonar data for superior accuracy! See the [Machine Learning Detection](#machine-learning-detection) section or run `python quick_start_ml.py` to get started.

## Features

- **Data Processing**
  - Native support for Oculus `.oculus` binary files
  - SimpleFire V2 message parsing
  - Polar to Cartesian coordinate conversion
  - Frame-by-frame processing

- **Filtering & Enhancement**
  - Median filtering for speckle noise reduction
  - Bilateral filtering for edge-preserving smoothing
  - Adaptive thresholding for segmentation
  - Range-dependent attenuation correction (TVG)
  - Water column noise removal
  - Contrast enhancement (CLAHE)
  - Beam pattern correction

- **Object Detection Methods**
  - **Classical CV**: Contour-based detection with morphological operations
  - **Edge-based**: Canny edge detection with contour analysis
  - **Clustering**: DBSCAN-based clustering of high-intensity regions
  - **Machine Learning**: YOLOv8-based detection with custom training
  - **Specialized**: Physics-based detection for spheres, barrels, cubes with shadow analysis
  - **Hybrid**: Intelligent fusion of ML and classical methods
  - **Multi-method fusion**: Weighted, voting, or NMS-based combination

- **Visualization**
  - Real-time display with OpenCV
  - Static frame visualization with matplotlib
  - Polar coordinate plotting
  - 3D point cloud visualization (Plotly)
  - Detection overlay and bounding boxes
  - Statistical analysis and heatmaps

## Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd OculusSonar-ObjectDetection
```

2. Install core dependencies:
```bash
pip install -r requirements.txt
```

3. For machine learning detection (recommended):
```bash
# CPU-only version
pip install torch torchvision ultralytics --index-url https://download.pytorch.org/whl/cpu

# GPU version (CUDA 11.8)
pip install torch torchvision ultralytics --index-url https://download.pytorch.org/whl/cu118
```

4. Verify installation:
```bash
python quick_start_ml.py  # Checks dependencies and sets up sample data
```

## Quick Start

### Basic Usage

Process a sonar file with default settings:
```bash
python main.py raw_assets/Oculus_20250805_164829.oculus
```

### Processing Modes

1. **Process only** - Run detection without visualization:
```bash
python main.py <file.oculus> --mode process
```

2. **Visualize specific frames**:
```bash
python main.py <file.oculus> --mode visualize --frames 0 10 20
```

3. **Real-time visualization**:
```bash
python main.py <file.oculus> --mode realtime
```
Press 'q' to quit, 's' to save current frame

4. **Analyze detections**:
```bash
python main.py <file.oculus> --mode analyze
```

5. **Full pipeline** (default):
```bash
python main.py <file.oculus> --mode all --save-outputs
```

### Using Configuration Files

Create a custom configuration:
```bash
python main.py <file.oculus> --config my_config.yaml
```

See `config.yaml` for all available options.

## Machine Learning Detection

### Overview

The system includes a complete machine learning pipeline using YOLOv8, optimized specifically for sonar imagery. This provides superior detection accuracy compared to classical methods alone.

### Quick ML Setup

```bash
# 1. Extract sample frames from your sonar files
python quick_start_ml.py

# 2. Annotate your data
python src/ml_detector/data_annotator.py

# 3. Train your model
python src/ml_detector/train_sonar_detector.py --data training_data --epochs 100

# 4. Use your model
python main.py raw_assets/your_file.oculus --config ml_config.yaml
```

### Detailed ML Workflow

#### Step 1: Prepare Training Data

Extract frames from your sonar recordings:

```python
from src.sonar_processor import OculusFileReader
import cv2

# Extract frames
reader = OculusFileReader("raw_assets/your_sonar_file.oculus")
frames = reader.parse_all_frames()

# Save frames for annotation
for i, frame in enumerate(frames[::10]):  # Every 10th frame
    img = cv2.normalize(frame.intensity_data, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)
    cv2.imwrite(f"training_data/raw_images/frame_{i:04d}.png", img)
```

#### Step 2: Annotate Your Data

Use the interactive annotation tool:

```bash
python src/ml_detector/data_annotator.py
```

**Annotation Controls:**
- **Click & Drag**: Draw bounding boxes
- **1-6 Keys**: Select object class (sphere, barrel, cube, fish, debris, object)
- **S**: Save and continue to next image
- **U**: Undo last box
- **C**: Clear all boxes
- **Space**: Skip image
- **Q**: Quit

**Best Practices:**
- Draw tight boxes around bright sonar returns
- Include shadow regions (they indicate 3D structure)
- Aim for 100-200 annotations minimum
- Be consistent in your labeling

#### Step 3: Train Your Model

**Basic Training:**
```bash
python src/ml_detector/train_sonar_detector.py --data training_data
```

**Advanced Training with Custom Parameters:**
```python
from src.ml_detector.train_sonar_detector import SonarModelTrainer

trainer = SonarModelTrainer("training_data")
trainer.config.update({
    'epochs': 150,              # More epochs for better accuracy
    'batch_size': 16,           # Adjust based on GPU memory
    'learning_rate': 0.01,      # Initial learning rate
    'conf_threshold': 0.25,     # Lower for sonar (harder detection)
})

# Train and evaluate
trainer.train()
metrics = trainer.evaluate()
print(f"mAP50: {metrics.box.map50:.3f}")

# Export model
model_path = trainer.export_model('onnx')  # For deployment
```

#### Step 4: Configure ML Detection

Update your `config.yaml` or create a new `ml_config.yaml`:

```yaml
detection:
  # ML Configuration
  use_ml: true                          # Enable ML detection
  use_hybrid: true                       # Combine ML + Classical (recommended)
  ml_model_path: "models/your_model.pt" # Path to trained model
  ml_confidence_threshold: 0.25         # Lower than typical CV (0.5)
  ml_iou_threshold: 0.45                # NMS threshold
  fusion_method: 'weighted'             # 'weighted', 'voting', or 'nms'
  
  # Classical fallback
  use_specialized: true                 # Physics-based detection
  confidence_threshold: 0.5
  min_area: 100
```

#### Step 5: Run Detection

**Command Line:**
```bash
python main.py raw_assets/sonar_file.oculus --config ml_config.yaml --save-outputs
```

**Python API:**
```python
from src.ml_detector.ml_sonar_detector import MLSonarDetector

# Initialize ML detector
detector = MLSonarDetector(
    model_path="models/your_model.pt",
    confidence_threshold=0.25,
    device='cuda'  # Use 'cpu' if no GPU
)

# Detect objects
detections = detector.detect(sonar_image, preprocess=True)
```

### Model Validation

Test your model performance:

```bash
# Validate on test dataset
python src/ml_detector/validate_model.py --model models/your_model.pt --dataset test_data

# Test on live sonar data
python src/ml_detector/validate_model.py --model models/your_model.pt --sonar-file test.oculus

# Compare detection methods
python src/ml_detector/validate_model.py --model models/your_model.pt --compare test_image.png
```

### Hybrid Detection

The hybrid detector combines ML with physics-based methods for best results:

```python
from src.ml_detector.ml_sonar_detector import HybridSonarDetector

detector = HybridSonarDetector(
    ml_model_path="models/your_model.pt",
    use_classical=True,      # Include classical CV
    use_specialized=True,    # Include shape-specific detection
    fusion_method='weighted' # How to combine results
)

detections = detector.detect(sonar_image)
```

### Performance Optimization

**GPU Acceleration:**
```python
# Automatically uses GPU if available
detector = MLSonarDetector(model_path, device='cuda')
```

**Batch Processing:**
```python
# Process multiple frames at once
detections_list = detector.detect_batch(image_list)
```

**Model Warmup:**
```python
# Warmup for consistent timing
detector.warmup(image_size=(640, 640))
```

### Sonar-Specific ML Features

1. **Preprocessing Pipeline:**
   - CLAHE for contrast enhancement
   - Bilateral filtering for noise reduction
   - Automatic grayscale-to-RGB conversion

2. **Custom Augmentations:**
   - Sonar noise patterns
   - Intensity variations (gain changes)
   - Motion blur (platform movement)
   - No color augmentations (sonar is grayscale)

3. **Optimized Hyperparameters:**
   - Lower confidence thresholds (0.25 vs 0.5)
   - Sonar-specific IoU thresholds
   - Custom loss weightings

### Troubleshooting ML Detection

**Low Detection Rate:**
- Lower confidence threshold (try 0.15-0.20)
- Ensure training data matches deployment conditions
- Check preprocessing settings
- Add more diverse training examples

**Too Many False Positives:**
- Increase confidence threshold
- Use hybrid detector with voting
- Add more negative examples to training
- Enable NMS with lower IoU threshold

**Model Not Learning:**
- Verify annotations are correct
- Ensure >100 training examples
- Check image normalization
- Try smaller learning rate

### Expected Performance

With proper training (100-200 annotations):
- **Precision**: 70-85%
- **Recall**: 65-80%
- **Speed**: 10-30 FPS (GPU), 2-5 FPS (CPU)
- **mAP50**: 0.60-0.75

## Project Structure

```
OculusSonar-ObjectDetection/
â”œâ”€â”€ main.py                     # Main application entry point
â”œâ”€â”€ config.yaml                 # Default configuration
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ ML_DETECTION_GUIDE.md       # Detailed ML guide
â”œâ”€â”€ quick_start_ml.py           # Quick ML setup script
â”œâ”€â”€ raw_assets/                 # Oculus sonar data files (.oculus)
â”œâ”€â”€ outputs/                    # Generated outputs (detections, visualizations)
â”œâ”€â”€ training_data/              # ML training data (created by user)
â”‚   â”œâ”€â”€ raw_images/            # Original sonar images
â”‚   â”œâ”€â”€ images/                # Annotated images
â”‚   â”œâ”€â”€ labels/                # YOLO format annotations
â”‚   â””â”€â”€ classes.txt            # Object class definitions
â”œâ”€â”€ models/                     # Trained ML models (created after training)
â””â”€â”€ src/
    â”œâ”€â”€ sonar_processor.py      # Oculus file reader and data structures
    â”œâ”€â”€ sonar_filters.py        # Filtering and enhancement algorithms
    â”œâ”€â”€ object_detector.py      # Classical detection algorithms
    â”œâ”€â”€ specialized_detector.py # Physics-based shape detection
    â”œâ”€â”€ visualizer.py           # Visualization utilities
    â””â”€â”€ ml_detector/           # Machine Learning module
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ data_annotator.py   # Interactive annotation tool
        â”œâ”€â”€ train_sonar_detector.py  # Training pipeline
        â”œâ”€â”€ ml_sonar_detector.py     # ML inference and hybrid detection
        â””â”€â”€ validate_model.py   # Model validation utilities
```

## API Usage

### Loading and Processing Data

```python
from src.sonar_processor import OculusFileReader
from src.sonar_filters import SonarEnhancer, create_default_pipeline
from src.object_detector import SonarObjectDetector

# Load sonar data
reader = OculusFileReader("path/to/file.oculus")
frames = reader.parse_all_frames()

# Setup enhancement pipeline
enhancer = create_default_pipeline()

# Setup detector
detector = SonarObjectDetector(methods=['classical', 'edge'])

# Process a frame
frame = frames[0]
enhanced = enhancer.process(frame.intensity_data)
detections = detector.detect(enhanced, frame.frame_index)

# Print results
for det in detections:
    print(f"Object at {det.centroid} with confidence {det.confidence:.2f}")
```

### Custom Filtering Pipeline

```python
from src.sonar_filters import SonarEnhancer, MedianFilter, BilateralFilter

# Create custom pipeline
enhancer = SonarEnhancer()
enhancer.add_filter(MedianFilter(kernel_size=5))
enhancer.add_filter(BilateralFilter(d=9, sigma_color=75, sigma_space=75))

# Apply to data
filtered = enhancer.process(sonar_data)
```

### Real-time Processing

```python
from src.visualizer import RealTimeVisualizer

# Setup real-time visualization
viz = RealTimeVisualizer()
viz.start()

# Process frames in real-time
for frame in frames:
    enhanced, detections = process_frame(frame)
    if not viz.update(enhanced, detections):
        break  # User pressed 'q'

viz.stop()
```

## Detection Algorithms

### Machine Learning (YOLOv8)
- **State-of-the-art** object detection using custom-trained models
- **Sonar-optimized**: CLAHE preprocessing, custom augmentations
- **High accuracy**: 70-85% precision with proper training
- **Multi-class**: Can distinguish spheres, barrels, cubes, fish, debris
- **GPU accelerated**: 10-30 FPS on CUDA-enabled systems

### Specialized Shape Detection
- **Physics-based** detection for specific underwater objects
- **Shadow analysis**: Uses acoustic shadows to confirm 3D volume
- **Shape-specific**: Optimized for spheres (crescents), barrels (ellipses), cubes (corners)
- **High confidence**: Combines brightness patterns with shadow validation

### Hybrid Detection
- **Intelligent fusion** of ML and classical methods
- **Weighted combination**: Balances strengths of each approach
- **Voting system**: Requires multiple detectors to agree
- **Best overall performance**: Highest accuracy and robustness

### Classical Computer Vision
- Uses adaptive thresholding and contour detection
- Best for high-contrast objects with clear boundaries
- Fast and reliable for simple scenes
- Good fallback when ML model unavailable

### Edge-based Detection
- Applies Canny edge detection
- Good for objects with strong edges
- Works well in noisy environments
- Lightweight and fast

### Clustering (DBSCAN)
- Groups high-intensity pixels into clusters
- Effective for irregular shaped objects
- Handles overlapping objects well
- No training required

## Configuration Options

Key parameters in `config.yaml`:

- **Filtering**
  - `median_kernel`: Size of median filter (reduce speckle)
  - `bilateral_*`: Edge-preserving smoothing parameters
  - `normalize_percentiles`: Intensity normalization range
  - `remove_water_column`: Bins to remove near sonar

- **Detection**
  - `methods`: List of classical detection algorithms to use
  - `combine_method`: How to merge multiple detections
  - `confidence_threshold`: Minimum detection confidence
  - `use_specialized`: Enable physics-based shape detection
  - `min_area`: Minimum object size in pixels

- **Machine Learning**
  - `use_ml`: Enable ML-based detection
  - `use_hybrid`: Use hybrid ML+Classical fusion
  - `ml_model_path`: Path to trained YOLOv8 model
  - `ml_confidence_threshold`: ML confidence (default 0.25)
  - `ml_iou_threshold`: NMS threshold (default 0.45)
  - `fusion_method`: How to combine detectors ('weighted', 'voting', 'nms')

- **Visualization**
  - `colormap`: Color scheme for sonar display
  - `show_realtime`: Enable live visualization
  - `save_outputs`: Save results to disk

## Output Format

Detection results are saved as JSON:
```json
{
  "frame_index": 0,
  "num_detections": 2,
  "detections": [
    {
      "bbox": [100, 50, 50, 50],
      "confidence": 0.85,
      "class_name": "object",
      "centroid": [125.0, 75.0],
      "area": 2500.0,
      "intensity_mean": 200.0,
      "intensity_std": 10.0
    }
  ]
}
```

## Performance Tips

1. **For large files**: Process in batches using frame indices
2. **For real-time**: Use fewer detection methods (e.g., only 'classical')
3. **For accuracy**: Use all methods with 'vote' combination
4. **For speed**: Reduce filter kernel sizes and disable bilateral filter

## Troubleshooting

### Common Issues

1. **"File not found"**: Check file path and ensure .oculus extension
2. **Low detection rate**: Adjust `confidence_threshold` and filter parameters
3. **Too many false positives**: Increase `min_area` and use 'intersection' combination
4. **Slow processing**: Reduce number of detection methods or filter complexity

### Debug Mode

Enable detailed logging:
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## Future Enhancements

- [x] ~~Train custom YOLO model on sonar data~~ âœ… **Implemented**
- [x] ~~Machine learning detection pipeline~~ âœ… **Implemented**
- [x] ~~Hybrid ML+Classical fusion~~ âœ… **Implemented**
- [x] ~~Interactive annotation tool~~ âœ… **Implemented**
- [ ] Object tracking across frames (Kalman filter/SORT)
- [ ] 3D reconstruction from sonar shadows
- [ ] Implement SLAM for mapping
- [ ] Real-time streaming from sonar hardware
- [ ] GPU acceleration for filtering operations
- [ ] Export to GeoTIFF with georeferencing
- [ ] Multi-beam sonar support
- [ ] Automatic model retraining pipeline
- [ ] Web-based annotation interface
- [ ] Docker containerization for deployment

## Contributing

Contributions are welcome! Please follow these steps:
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## License

This project is licensed under the MIT License.

## Acknowledgments

- Blueprint Subsea for Oculus SDK documentation
- ROS2 oculus_replay_node for file format insights
- OpenCV and scikit-image communities

## Contact

For questions or support, please open an issue on GitHub.